from typing import Dict, List, Tuple, Any
import numpy as np
from scipy import stats
from dataclasses import dataclass

from ...models.metrics import MetricType

@dataclass
class ExperimentResult:
    metric_name: str
    control_mean: float
    variant_mean: float
    relative_difference: float
    p_value: float
    confidence_interval: Tuple[float, float]
    sample_size: Dict[str, int]
    power: float
    is_significant: bool

class StatisticalAnalysisService:
    def __init__(self, min_sample_size: int = 100, confidence_level: float = 0.95):
        if not 0 < confidence_level < 1:
            raise ValueError("Confidence level must be between 0 and 1")
        if min_sample_size < 1:
            raise ValueError("Minimum sample size must be positive")
            
        self.min_sample_size = min_sample_size
        self.confidence_level = confidence_level
        self.alpha = 1 - confidence_level

    def _validate_input_data(self, control_data: List[float], variant_data: List[float], metric_type: MetricType):
        """Validate input data based on metric type and requirements"""
        if not control_data or not variant_data:
            raise ValueError("Control and variant data cannot be empty")
            
        if len(control_data) < self.min_sample_size or len(variant_data) < self.min_sample_size:
            raise ValueError(f"Insufficient sample size. Minimum required: {self.min_sample_size}")

        if metric_type == MetricType.BINARY:
            if not all(x in (0, 1) for x in control_data + variant_data):
                raise ValueError("Binary metrics must contain only 0s and 1s")
                
        if metric_type == MetricType.COUNT:
            if not all(isinstance(x, (int, np.integer)) and x >= 0 for x in control_data + variant_data):
                raise ValueError("Count metrics must contain non-negative integers")
                
        if metric_type == MetricType.RATIO:
            if not all(0 <= x <= 1 for x in control_data + variant_data):
                raise ValueError("Ratio metrics must contain values between 0 and 1")

    async def analyze_experiment(
        self,
        control_data: List[float],
        variant_data: List[float],
        metric_type: MetricType,
        metric_name: str
    ) -> ExperimentResult:
        """Perform statistical analysis on experiment data"""
        self._validate_input_data(control_data, variant_data, metric_type)
        
        control_mean = np.mean(control_data)
        variant_mean = np.mean(variant_data)
        relative_diff = ((variant_mean - control_mean) / control_mean) * 100

        if metric_type == MetricType.BINARY:
            p_value = self._calculate_binary_significance(control_data, variant_data)
            ci = self._calculate_binary_confidence_interval(control_data, variant_data)
        elif metric_type == MetricType.COUNT:
            _, p_value = stats.mannwhitneyu(control_data, variant_data, alternative='two-sided')
            ci = self._calculate_continuous_confidence_interval(control_data, variant_data)
        elif metric_type == MetricType.RATIO:
            t_stat, p_value = stats.ttest_ind(control_data, variant_data)
            ci = self._calculate_ratio_confidence_interval(control_data, variant_data)
        else:
            t_stat, p_value = stats.ttest_ind(control_data, variant_data)
            ci = self._calculate_continuous_confidence_interval(control_data, variant_data)

        power = self._calculate_power(control_data, variant_data)

        return ExperimentResult(
            metric_name=metric_name,
            control_mean=control_mean,
            variant_mean=variant_mean,
            relative_difference=relative_diff,
            p_value=p_value,
            confidence_interval=ci,
            sample_size={
                "control": len(control_data),
                "variant": len(variant_data)
            },
            power=power,
            is_significant=p_value < self.alpha
        )

    def _calculate_binary_significance(self, control_data: List[float], variant_data: List[float]) -> float:
        """Calculate significance for binary metrics using chi-square test"""
        control_success = sum(control_data)
        variant_success = sum(variant_data)
        
        contingency_table = np.array([
            [control_success, len(control_data) - control_success],
            [variant_success, len(variant_data) - variant_success]
        ])
        
        _, p_value = stats.chi2_contingency(contingency_table)
        return p_value

    def _calculate_binary_confidence_interval(
        self,
        control_data: List[float],
        variant_data: List[float]
    ) -> Tuple[float, float]:
        """Calculate confidence interval for binary metrics"""
        control_prop = np.mean(control_data)
        variant_prop = np.mean(variant_data)
        
        se = np.sqrt(
            (control_prop * (1 - control_prop) / len(control_data)) +
            (variant_prop * (1 - variant_prop) / len(variant_data))
        )
        
        z_score = stats.norm.ppf(1 - self.alpha / 2)
        margin_error = z_score * se
        
        return (variant_prop - control_prop - margin_error,
                variant_prop - control_prop + margin_error)

    def _calculate_continuous_confidence_interval(
        self,
        control_data: List[float],
        variant_data: List[float]
    ) -> Tuple[float, float]:
        """Calculate confidence interval for continuous metrics"""
        control_mean = np.mean(control_data)
        variant_mean = np.mean(variant_data)
        
        control_var = np.var(control_data, ddof=1)
        variant_var = np.var(variant_data, ddof=1)
        
        se = np.sqrt(
            (control_var / len(control_data)) +
            (variant_var / len(variant_data))
        )
        
        t_score = stats.t.ppf(1 - self.alpha / 2, len(control_data) + len(variant_data) - 2)
        margin_error = t_score * se
        
        return (variant_mean - control_mean - margin_error,
                variant_mean - control_mean + margin_error)

    def _calculate_power(self, control_data: List[float], variant_data: List[float]) -> float:
        """Calculate statistical power"""
        effect_size = self._cohens_d(control_data, variant_data)
        return stats.power.TTestIndPower().power(
            effect_size=effect_size,
            nobs=min(len(control_data), len(variant_data)),
            alpha=self.alpha
        )

    def _cohens_d(self, control_data: List[float], variant_data: List[float]) -> float:
        """Calculate Cohen's d effect size"""
        n1, n2 = len(control_data), len(variant_data)
        var1, var2 = np.var(control_data, ddof=1), np.var(variant_data, ddof=1)
        
        pooled_sd = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
        
        return abs(np.mean(control_data) - np.mean(variant_data)) / pooled_sd

    def _calculate_ratio_confidence_interval(
        self,
        control_data: List[float],
        variant_data: List[float]
    ) -> Tuple[float, float]:
        """Calculate confidence interval for ratio metrics, ensuring bounds are [0,1]"""
        ci_low, ci_high = self._calculate_continuous_confidence_interval(control_data, variant_data)
        return (max(0.0, ci_low), min(1.0, ci_high))

    def get_recommended_duration(
        self,
        daily_samples: int,
        required_sample_size: int
    ) -> int:
        """Calculate recommended experiment duration in days"""
        return int(np.ceil(required_sample_size / daily_samples))

    def estimate_mde(
        self,
        baseline_rate: float,
        sample_size: int,
        power: float = 0.8
    ) -> float:
        """Estimate minimum detectable effect given a sample size"""
        analysis = stats.power.TTestIndPower()
        effect_size = analysis.solve_effect_size(
            nobs=sample_size,
            alpha=self.alpha,
            power=power
        )
        return effect_size * np.sqrt(baseline_rate * (1 - baseline_rate))

    async def calculate_sample_size(
        self,
        baseline_rate: float,
        minimum_detectable_effect: float,
        power: float = 0.8
    ) -> int:
        """Calculate required sample size for desired statistical power"""
        effect_size = minimum_detectable_effect / np.sqrt(
            baseline_rate * (1 - baseline_rate)
        )
        
        analysis = stats.power.TTestIndPower()
        sample_size = analysis.solve_power(
            effect_size=effect_size,
            power=power,
            alpha=self.alpha
        )
        
        return int(np.ceil(sample_size))

    def _check_sequential_stopping(
        self,
        control_data: List[float],
        variant_data: List[float],
        metric_type: MetricType,
        stopping_threshold: float
    ) -> Tuple[bool, Dict[str, Any]]:
        """Check if sequential testing should stop early"""
        if len(control_data) < self.min_sample_size or len(variant_data) < self.min_sample_size:
            return False, {}

        if metric_type == MetricType.BINARY:
            control_mean = np.mean(control_data)
            variant_mean = np.mean(variant_data)
            pooled_p = (sum(control_data) + sum(variant_data)) / (len(control_data) + len(variant_data))
            se = np.sqrt(pooled_p * (1 - pooled_p) * (1/len(control_data) + 1/len(variant_data)))
        else:
            control_mean = np.mean(control_data)
            variant_mean = np.mean(variant_data)
            pooled_std = np.sqrt(
                ((len(control_data) - 1) * np.var(control_data, ddof=1) + 
                 (len(variant_data) - 1) * np.var(variant_data, ddof=1)) / 
                (len(control_data) + len(variant_data) - 2)
            )
            se = pooled_std * np.sqrt(1/len(control_data) + 1/len(variant_data))

        z_score = (variant_mean - control_mean) / se
        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

        should_stop = p_value < stopping_threshold or p_value > (1 - stopping_threshold)
        
        return should_stop, {
            "z_score": z_score,
            "p_value": p_value,
            "control_mean": control_mean,
            "variant_mean": variant_mean,
            "standard_error": se
        } 